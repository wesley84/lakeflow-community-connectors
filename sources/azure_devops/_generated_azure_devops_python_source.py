# ==============================================================================
# Merged Lakeflow Source: azure_devops
# ==============================================================================
# This file is auto-generated by tools/scripts/merge_python_source.py
# Do not edit manually. Make changes to the source files instead.
# ==============================================================================

from datetime import datetime
from decimal import Decimal
from typing import Any, Iterator
import json

from pyspark.sql import Row
from pyspark.sql.datasource import DataSource, DataSourceReader, SimpleDataSourceStreamReader
from pyspark.sql.types import *
import base64
import requests


def register_lakeflow_source(spark):
    """Register the Lakeflow Python source with Spark."""

    ########################################################
    # libs/utils.py
    ########################################################

    def _parse_struct(value: Any, field_type: StructType) -> Row:
        """Parse a dictionary into a PySpark Row based on StructType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for StructType, got {type(value)}")
        # Spark Python -> Arrow conversion require missing StructType fields to be assigned None.
        if value == {}:
            raise ValueError(
                "field in StructType cannot be an empty dict. "
                "Please assign None as the default value instead."
            )
        field_dict = {}
        for field in field_type.fields:
            if field.name in value:
                field_dict[field.name] = parse_value(value.get(field.name), field.dataType)
            elif field.nullable:
                field_dict[field.name] = None
            else:
                raise ValueError(f"Field {field.name} is not nullable but not found in the input")
        return Row(**field_dict)


    def _parse_array(value: Any, field_type: ArrayType) -> list:
        """Parse a list into a PySpark array based on ArrayType schema."""
        if not isinstance(value, list):
            if field_type.containsNull:
                return [parse_value(value, field_type.elementType)]
            raise ValueError(f"Expected a list for ArrayType, got {type(value)}")
        return [parse_value(v, field_type.elementType) for v in value]


    def _parse_map(value: Any, field_type: MapType) -> dict:
        """Parse a dictionary into a PySpark map based on MapType schema."""
        if not isinstance(value, dict):
            raise ValueError(f"Expected a dictionary for MapType, got {type(value)}")
        return {
            parse_value(k, field_type.keyType): parse_value(v, field_type.valueType)
            for k, v in value.items()
        }


    def _parse_string(value: Any) -> str:
        """Convert value to string."""
        return str(value)


    def _parse_integer(value: Any) -> int:
        """Convert value to integer."""
        if isinstance(value, str) and value.strip():
            return int(float(value)) if "." in value else int(value)
        if isinstance(value, (int, float)):
            return int(value)
        raise ValueError(f"Cannot convert {value} to integer")


    def _parse_float(value: Any) -> float:
        """Convert value to float."""
        return float(value)


    def _parse_decimal(value: Any) -> Decimal:
        """Convert value to Decimal."""
        return Decimal(value) if isinstance(value, str) and value.strip() else Decimal(str(value))


    def _parse_boolean(value: Any) -> bool:
        """Convert value to boolean."""
        if isinstance(value, str):
            lowered = value.lower()
            if lowered in ("true", "t", "yes", "y", "1"):
                return True
            if lowered in ("false", "f", "no", "n", "0"):
                return False
        return bool(value)


    def _parse_date(value: Any) -> datetime.date:
        """Convert value to date."""
        if isinstance(value, str):
            for fmt in ("%Y-%m-%d", "%m/%d/%Y", "%d-%m-%Y", "%Y/%m/%d"):
                try:
                    return datetime.strptime(value, fmt).date()
                except ValueError:
                    continue
            return datetime.fromisoformat(value).date()
        if isinstance(value, datetime):
            return value.date()
        raise ValueError(f"Cannot convert {value} to date")


    def _parse_timestamp(value: Any) -> datetime:
        """Convert value to timestamp."""
        if isinstance(value, str):
            ts_value = value.replace("Z", "+00:00") if value.endswith("Z") else value
            try:
                return datetime.fromisoformat(ts_value)
            except ValueError:
                for fmt in ("%Y-%m-%d %H:%M:%S", "%Y/%m/%d %H:%M:%S"):
                    try:
                        return datetime.strptime(ts_value, fmt)
                    except ValueError:
                        continue
        elif isinstance(value, (int, float)):
            return datetime.fromtimestamp(value)
        elif isinstance(value, datetime):
            return value
        raise ValueError(f"Cannot convert {value} to timestamp")


    def _decode_string_to_bytes(value: str) -> bytes:
        """Try to decode a string as base64, then hex, then UTF-8."""
        try:
            return base64.b64decode(value)
        except Exception:
            pass
        try:
            return bytes.fromhex(value)
        except Exception:
            pass
        return value.encode("utf-8")


    def _parse_binary(value: Any) -> bytes:
        """Convert value to bytes. Tries base64, then hex, then UTF-8 for strings."""
        if isinstance(value, bytes):
            return value
        if isinstance(value, bytearray):
            return bytes(value)
        if isinstance(value, str):
            return _decode_string_to_bytes(value)
        if isinstance(value, list):
            return bytes(value)
        return str(value).encode("utf-8")


    # Mapping of primitive types to their parser functions
    _PRIMITIVE_PARSERS = {
        StringType: _parse_string,
        IntegerType: _parse_integer,
        LongType: _parse_integer,
        FloatType: _parse_float,
        DoubleType: _parse_float,
        DecimalType: _parse_decimal,
        BooleanType: _parse_boolean,
        DateType: _parse_date,
        TimestampType: _parse_timestamp,
        BinaryType: _parse_binary,
    }


    def parse_value(value: Any, field_type: DataType) -> Any:
        """
        Converts a JSON value into a PySpark-compatible data type based on the provided field type.
        """
        if value is None:
            return None

        # Handle complex types
        if isinstance(field_type, StructType):
            return _parse_struct(value, field_type)
        if isinstance(field_type, ArrayType):
            return _parse_array(value, field_type)
        if isinstance(field_type, MapType):
            return _parse_map(value, field_type)

        # Handle primitive types via type-based lookup
        try:
            field_type_class = type(field_type)
            if field_type_class in _PRIMITIVE_PARSERS:
                return _PRIMITIVE_PARSERS[field_type_class](value)

            # Check for custom UDT handling
            if hasattr(field_type, "fromJson"):
                return field_type.fromJson(value)

            raise TypeError(f"Unsupported field type: {field_type}")
        except (ValueError, TypeError) as e:
            raise ValueError(f"Error converting '{value}' ({type(value)}) to {field_type}: {str(e)}")


    ########################################################
    # sources/azure_devops/azure_devops.py
    ########################################################

    class LakeflowConnect:
        def __init__(self, options: dict[str, str]) -> None:
            """
            Initialize the Azure DevOps connector with connection-level options.

            Expected options:
                - organization: Azure DevOps organization name.
                - project: Project name or ID (optional for organization-level objects like users).
                - personal_access_token: Personal access token (PAT) for authentication.
            """
            organization = options.get("organization")
            project = options.get("project")
            personal_access_token = options.get("personal_access_token")

            if not organization:
                raise ValueError(
                    "Azure DevOps connector requires 'organization' in options"
                )
            # Note: project is optional for organization-level objects (e.g., users)
            if not personal_access_token:
                raise ValueError(
                    "Azure DevOps connector requires 'personal_access_token' in options"
                )

            self.organization = organization
            self.project = project  # May be None for organization-level objects
            self.base_url = f"https://dev.azure.com/{organization}"
            self.vssps_base_url = f"https://vssps.dev.azure.com/{organization}"

            # Encode PAT as Base64 in the format :{pat}
            auth_string = f":{personal_access_token}"
            auth_bytes = auth_string.encode("ascii")
            base64_auth = base64.b64encode(auth_bytes).decode("ascii")

            # Configure a session with proper headers for Azure DevOps REST API
            self._session = requests.Session()
            self._session.headers.update(
                {
                    "Authorization": f"Basic {base64_auth}",
                    "Accept": "application/json",
                }
            )

        def list_tables(self) -> list[str]:
            """
            List names of all tables supported by this connector.

            Supported tables:
            - projects: Projects within an organization
            - repositories: Git repositories within a project
            - commits: Git commits across repositories
            - pullrequests: Pull requests across repositories
            - pullrequest_threads: Comments and discussions on pull requests
            - pullrequest_workitems: Work items linked to pull requests
            - pullrequest_commits: Commits included in pull requests
            - pullrequest_reviewers: Detailed reviewer information for pull requests
            - refs: Git references (branches and tags)
            - pushes: Git push events to repositories
            - users: User identities and profiles in the organization
            - workitems: Work items (tasks, bugs, user stories, etc.)
            - workitem_revisions: Historical changes to work items
            - workitem_types: Work item type definitions and fields
            """
            return [
                "projects",
                "repositories",
                "commits",
                "pullrequests",
                "pullrequest_threads",
                "pullrequest_workitems",
                "pullrequest_commits",
                "pullrequest_reviewers",
                "refs",
                "pushes",
                "users",
                "workitems",
                "workitem_revisions",
                "workitem_types",
            ]

        def get_table_schema(
            self, table_name: str, table_options: dict[str, str]
        ) -> StructType:
            """
            Fetch the schema of a table.

            The schema is static and derived from the Azure DevOps REST API documentation
            and connector design for the `repositories` object.
            """
            if table_name not in self.list_tables():
                raise ValueError(f"Unsupported table: {table_name!r}")

            # Nested `project` struct schema
            project_struct = StructType(
                [
                    StructField("id", StringType(), True),
                    StructField("name", StringType(), True),
                    StructField("url", StringType(), True),
                    StructField("state", StringType(), True),
                    StructField("revision", LongType(), True),
                    StructField("visibility", StringType(), True),
                    StructField("lastUpdateTime", StringType(), True),
                ]
            )

            # Nested `parentRepository` struct schema (only present for forks)
            parent_repository_struct = StructType(
                [
                    StructField("id", StringType(), True),
                    StructField("name", StringType(), True),
                    StructField("url", StringType(), True),
                    StructField("project", project_struct, True),
                ]
            )

            # Nested `_links` struct schema (HAL-style hypermedia links)
            # Each link has an 'href' field
            link_struct = StructType([StructField("href", StringType(), True)])

            links_struct = StructType(
                [
                    StructField("self", link_struct, True),
                    StructField("project", link_struct, True),
                    StructField("web", link_struct, True),
                    StructField("ssh", link_struct, True),
                    StructField("commits", link_struct, True),
                    StructField("refs", link_struct, True),
                    StructField("pullRequests", link_struct, True),
                    StructField("items", link_struct, True),
                    StructField("pushes", link_struct, True),
                ]
            )

            # Nested identity struct (for createdBy, author, committer, pushedBy, etc.)
            identity_struct = StructType(
                [
                    StructField("id", StringType(), True),
                    StructField("displayName", StringType(), True),
                    StructField("uniqueName", StringType(), True),
                    StructField("url", StringType(), True),
                    StructField("imageUrl", StringType(), True),
                ]
            )

            # Nested author/committer struct (for commits)
            git_user_struct = StructType(
                [
                    StructField("name", StringType(), True),
                    StructField("email", StringType(), True),
                    StructField("date", StringType(), True),
                ]
            )

            # Nested changeCounts struct (for commits)
            change_counts_struct = StructType(
                [
                    StructField("Add", IntegerType(), True),
                    StructField("Edit", IntegerType(), True),
                    StructField("Delete", IntegerType(), True),
                ]
            )

            # Nested commit reference struct (for pull requests)
            commit_ref_struct = StructType(
                [
                    StructField("commitId", StringType(), True),
                    StructField("url", StringType(), True),
                ]
            )

            # Nested refUpdate struct (for pushes)
            ref_update_struct = StructType(
                [
                    StructField("name", StringType(), True),
                    StructField("oldObjectId", StringType(), True),
                    StructField("newObjectId", StringType(), True),
                ]
            )

            if table_name == "projects":
                projects_schema = StructType(
                    [
                        StructField("id", StringType(), False),
                        StructField("name", StringType(), True),
                        StructField("description", StringType(), True),
                        StructField("url", StringType(), True),
                        StructField("state", StringType(), True),
                        StructField("revision", LongType(), True),
                        StructField("visibility", StringType(), True),
                        StructField("lastUpdateTime", StringType(), True),
                        StructField("organization", StringType(), False),
                    ]
                )
                return projects_schema

            if table_name == "repositories":
                repositories_schema = StructType(
                    [
                        StructField("id", StringType(), False),
                        StructField("name", StringType(), True),
                        StructField("url", StringType(), True),
                        StructField("project", project_struct, True),
                        StructField("defaultBranch", StringType(), True),
                        StructField("size", LongType(), True),
                        StructField("remoteUrl", StringType(), True),
                        StructField("sshUrl", StringType(), True),
                        StructField("webUrl", StringType(), True),
                        StructField("isDisabled", BooleanType(), True),
                        StructField("isInMaintenance", BooleanType(), True),
                        StructField("isFork", BooleanType(), True),
                        StructField("parentRepository", parent_repository_struct, True),
                        StructField("_links", links_struct, True),
                        StructField("organization", StringType(), False),
                        StructField("project_name", StringType(), False),
                    ]
                )
                return repositories_schema

            if table_name == "commits":
                commits_schema = StructType(
                    [
                        StructField("commitId", StringType(), False),
                        StructField("author", git_user_struct, True),
                        StructField("committer", git_user_struct, True),
                        StructField("comment", StringType(), True),
                        StructField("commentTruncated", BooleanType(), True),
                        StructField("changeCounts", change_counts_struct, True),
                        StructField("url", StringType(), True),
                        StructField("remoteUrl", StringType(), True),
                        StructField("treeId", StringType(), True),
                        StructField("parents", ArrayType(StringType()), True),
                        StructField("organization", StringType(), False),
                        StructField("project_name", StringType(), False),
                        StructField("repository_id", StringType(), False),
                    ]
                )
                return commits_schema

            if table_name == "pullrequests":
                pullrequests_schema = StructType(
                    [
                        StructField("pullRequestId", LongType(), False),
                        StructField("codeReviewId", LongType(), True),
                        StructField("status", StringType(), True),
                        StructField("createdBy", identity_struct, True),
                        StructField("creationDate", StringType(), True),
                        StructField("closedDate", StringType(), True),
                        StructField("title", StringType(), True),
                        StructField("description", StringType(), True),
                        StructField("sourceRefName", StringType(), True),
                        StructField("targetRefName", StringType(), True),
                        StructField("mergeStatus", StringType(), True),
                        StructField("mergeId", StringType(), True),
                        StructField("lastMergeSourceCommit", commit_ref_struct, True),
                        StructField("lastMergeTargetCommit", commit_ref_struct, True),
                        StructField("lastMergeCommit", commit_ref_struct, True),
                        StructField("url", StringType(), True),
                        StructField("supportsIterations", BooleanType(), True),
                        StructField("artifactId", StringType(), True),
                        StructField("organization", StringType(), False),
                        StructField("project_name", StringType(), False),
                        StructField("repository_id", StringType(), False),
                    ]
                )
                return pullrequests_schema

            if table_name == "refs":
                refs_schema = StructType(
                    [
                        StructField("name", StringType(), False),
                        StructField("objectId", StringType(), True),
                        StructField("creator", identity_struct, True),
                        StructField("url", StringType(), True),
                        StructField("peeledObjectId", StringType(), True),
                        StructField("organization", StringType(), False),
                        StructField("project_name", StringType(), False),
                        StructField("repository_id", StringType(), False),
                    ]
                )
                return refs_schema

            if table_name == "pushes":
                pushes_schema = StructType(
                    [
                        StructField("pushId", LongType(), False),
                        StructField("date", StringType(), True),
                        StructField("pushedBy", identity_struct, True),
                        StructField("url", StringType(), True),
                        StructField("refUpdates", ArrayType(ref_update_struct), True),
                        StructField("organization", StringType(), False),
                        StructField("project_name", StringType(), False),
                        StructField("repository_id", StringType(), False),
                    ]
                )
                return pushes_schema

            if table_name == "users":
                users_schema = StructType(
                    [
                        StructField("descriptor", StringType(), False),
                        StructField("displayName", StringType(), True),
                        StructField("mailAddress", StringType(), True),
                        StructField("principalName", StringType(), True),
                        StructField("origin", StringType(), True),
                        StructField("originId", StringType(), True),
                        StructField("subjectKind", StringType(), True),
                        StructField("domain", StringType(), True),
                        StructField("directoryAlias", StringType(), True),
                        StructField("url", StringType(), True),
                        StructField("_links", links_struct, True),
                        StructField("organization", StringType(), False),
                    ]
                )
                return users_schema

            if table_name == "pullrequest_threads":
                # Nested comment struct
                comment_struct = StructType(
                    [
                        StructField("id", LongType(), True),
                        StructField("parentCommentId", LongType(), True),
                        StructField("author", identity_struct, True),
                        StructField("content", StringType(), True),
                        StructField("publishedDate", StringType(), True),
                        StructField("lastUpdatedDate", StringType(), True),
                        StructField("lastContentUpdatedDate", StringType(), True),
                        StructField("commentType", StringType(), True),
                    ]
                )

                # Nested threadContext struct
                position_struct = StructType(
                    [
                        StructField("line", LongType(), True),
                        StructField("offset", LongType(), True),
                    ]
                )

                thread_context_struct = StructType(
                    [
                        StructField("filePath", StringType(), True),
                        StructField("rightFileStart", position_struct, True),
                        StructField("rightFileEnd", position_struct, True),
                        StructField("leftFileStart", position_struct, True),
                        StructField("leftFileEnd", position_struct, True),
                    ]
                )

                pullrequest_threads_schema = StructType(
                    [
                        StructField("id", LongType(), False),
                        StructField("publishedDate", StringType(), True),
                        StructField("lastUpdatedDate", StringType(), True),
                        StructField("comments", ArrayType(comment_struct), True),
                        StructField("status", StringType(), True),
                        StructField("threadContext", thread_context_struct, True),
                        StructField("isDeleted", BooleanType(), True),
                        StructField("organization", StringType(), False),
                        StructField("project_name", StringType(), False),
                        StructField("repository_id", StringType(), False),
                        StructField("pullrequest_id", LongType(), False),
                    ]
                )
                return pullrequest_threads_schema

            if table_name == "pullrequest_workitems":
                pullrequest_workitems_schema = StructType(
                    [
                        StructField("id", StringType(), False),
                        StructField("url", StringType(), True),
                        StructField("organization", StringType(), False),
                        StructField("project_name", StringType(), False),
                        StructField("repository_id", StringType(), False),
                        StructField("pullrequest_id", LongType(), False),
                    ]
                )
                return pullrequest_workitems_schema

            if table_name == "pullrequest_commits":
                pullrequest_commits_schema = StructType(
                    [
                        StructField("commitId", StringType(), False),
                        StructField("author", git_user_struct, True),
                        StructField("committer", git_user_struct, True),
                        StructField("comment", StringType(), True),
                        StructField("commentTruncated", BooleanType(), True),
                        StructField("url", StringType(), True),
                        StructField("organization", StringType(), False),
                        StructField("project_name", StringType(), False),
                        StructField("repository_id", StringType(), False),
                        StructField("pullrequest_id", LongType(), False),
                    ]
                )
                return pullrequest_commits_schema

            if table_name == "pullrequest_reviewers":
                pullrequest_reviewers_schema = StructType(
                    [
                        StructField("reviewerUrl", StringType(), True),
                        StructField("vote", LongType(), True),
                        StructField("hasDeclined", BooleanType(), True),
                        StructField("isFlagged", BooleanType(), True),
                        StructField("displayName", StringType(), True),
                        StructField("id", StringType(), False),
                        StructField("uniqueName", StringType(), True),
                        StructField("url", StringType(), True),
                        StructField("imageUrl", StringType(), True),
                        StructField("isRequired", BooleanType(), True),
                        StructField("organization", StringType(), False),
                        StructField("project_name", StringType(), False),
                        StructField("repository_id", StringType(), False),
                        StructField("pullrequest_id", LongType(), False),
                    ]
                )
                return pullrequest_reviewers_schema

            if table_name == "workitems":
                # Nested relation struct
                relation_struct = StructType(
                    [
                        StructField("rel", StringType(), True),
                        StructField("url", StringType(), True),
                    ]
                )

                workitems_schema = StructType(
                    [
                        StructField("id", LongType(), False),
                        StructField("rev", LongType(), True),
                        StructField("fields", StringType(), True),  # JSON string for dynamic fields
                        StructField("relations", ArrayType(relation_struct), True),
                        StructField("url", StringType(), True),
                        StructField("organization", StringType(), False),
                        StructField("project_name", StringType(), False),
                    ]
                )
                return workitems_schema

            if table_name == "workitem_revisions":
                workitem_revisions_schema = StructType(
                    [
                        StructField("id", LongType(), False),
                        StructField("rev", LongType(), False),
                        StructField("workItemId", LongType(), True),
                        StructField("revisedDate", StringType(), True),
                        StructField("fields", StringType(), True),  # JSON string for dynamic fields
                        StructField("url", StringType(), True),
                        StructField("organization", StringType(), False),
                        StructField("project_name", StringType(), False),
                    ]
                )
                return workitem_revisions_schema

            if table_name == "workitem_types":
                # Nested field definition struct
                field_def_struct = StructType(
                    [
                        StructField("referenceName", StringType(), True),
                        StructField("name", StringType(), True),
                        StructField("type", StringType(), True),
                        StructField("readOnly", BooleanType(), True),
                        StructField("required", BooleanType(), True),
                        StructField("defaultValue", StringType(), True),
                        StructField("helpText", StringType(), True),
                    ]
                )

                # Nested state struct
                state_struct = StructType(
                    [
                        StructField("name", StringType(), True),
                        StructField("color", StringType(), True),
                        StructField("category", StringType(), True),
                    ]
                )

                # Icon struct
                icon_struct = StructType(
                    [
                        StructField("id", StringType(), True),
                        StructField("url", StringType(), True),
                    ]
                )

                workitem_types_schema = StructType(
                    [
                        StructField("name", StringType(), True),
                        StructField("referenceName", StringType(), False),
                        StructField("description", StringType(), True),
                        StructField("color", StringType(), True),
                        StructField("icon", icon_struct, True),
                        StructField("isDisabled", BooleanType(), True),
                        StructField("fields", ArrayType(field_def_struct), True),
                        StructField("states", ArrayType(state_struct), True),
                        StructField("url", StringType(), True),
                        StructField("organization", StringType(), False),
                        StructField("project_name", StringType(), False),
                    ]
                )
                return workitem_types_schema

            raise ValueError(f"Unsupported table: {table_name!r}")

        def read_table_metadata(
            self, table_name: str, table_options: dict[str, str]
        ) -> dict:
            """
            Fetch metadata for the given table.

            Metadata includes:
            - primary_keys: List of column names that uniquely identify a record
            - ingestion_type: One of 'snapshot', 'append', or 'cdc'
            - cursor_field: (optional) Field used for incremental ingestion
            """
            if table_name not in self.list_tables():
                raise ValueError(f"Unsupported table: {table_name!r}")

            if table_name == "projects":
                return {
                    "primary_keys": ["id"],
                    "ingestion_type": "snapshot",
                }

            if table_name == "repositories":
                return {
                    "primary_keys": ["id"],
                    "ingestion_type": "snapshot",
                }

            if table_name == "commits":
                return {
                    "primary_keys": ["commitId", "repository_id"],
                    "ingestion_type": "append",
                }

            if table_name == "pullrequests":
                return {
                    "primary_keys": ["pullRequestId", "repository_id"],
                    "cursor_field": "closedDate",
                    "ingestion_type": "cdc",
                }

            if table_name == "refs":
                return {
                    "primary_keys": ["name", "repository_id"],
                    "ingestion_type": "snapshot",
                }

            if table_name == "pushes":
                return {
                    "primary_keys": ["pushId", "repository_id"],
                    "ingestion_type": "append",
                }

            if table_name == "users":
                return {
                    "primary_keys": ["descriptor"],
                    "ingestion_type": "snapshot",
                }

            if table_name == "pullrequest_threads":
                return {
                    "primary_keys": ["id", "pullrequest_id", "repository_id"],
                    "ingestion_type": "append",
                    "cursor_field": "publishedDate",
                }

            if table_name == "pullrequest_workitems":
                return {
                    "primary_keys": ["id", "pullrequest_id", "repository_id"],
                    "ingestion_type": "snapshot",
                }

            if table_name == "pullrequest_commits":
                return {
                    "primary_keys": ["commitId", "pullrequest_id", "repository_id"],
                    "ingestion_type": "snapshot",
                }

            if table_name == "pullrequest_reviewers":
                return {
                    "primary_keys": ["id", "pullrequest_id", "repository_id"],
                    "ingestion_type": "snapshot",
                }

            if table_name == "workitems":
                return {
                    "primary_keys": ["id"],
                    "ingestion_type": "cdc",
                    "cursor_field": "rev",
                }

            if table_name == "workitem_revisions":
                return {
                    "primary_keys": ["id", "rev"],
                    "ingestion_type": "append",
                }

            if table_name == "workitem_types":
                return {
                    "primary_keys": ["referenceName", "project_name"],
                    "ingestion_type": "snapshot",
                }

            raise ValueError(f"Unsupported table: {table_name!r}")

        def read_table(
            self, table_name: str, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read records from a table and return raw JSON-like dictionaries.

            Table-specific requirements:
            - repositories: No table_options required
            - commits, pullrequests, refs, pushes: Require repository_id in table_options

            Returns:
            - Iterator of record dictionaries
            - Updated offset dictionary for pagination/incremental reads
            """
            if table_name not in self.list_tables():
                raise ValueError(f"Unsupported table: {table_name!r}")

            if table_name == "projects":
                return self._read_projects(start_offset, table_options)

            if table_name == "repositories":
                return self._read_repositories(start_offset, table_options)

            if table_name == "commits":
                return self._read_commits(start_offset, table_options)

            if table_name == "pullrequests":
                return self._read_pullrequests(start_offset, table_options)

            if table_name == "refs":
                return self._read_refs(start_offset, table_options)

            if table_name == "pushes":
                return self._read_pushes(start_offset, table_options)

            if table_name == "users":
                return self._read_users(start_offset, table_options)

            if table_name == "pullrequest_threads":
                return self._read_pullrequest_threads(start_offset, table_options)

            if table_name == "pullrequest_workitems":
                return self._read_pullrequest_workitems(start_offset, table_options)

            if table_name == "pullrequest_commits":
                return self._read_pullrequest_commits(start_offset, table_options)

            if table_name == "pullrequest_reviewers":
                return self._read_pullrequest_reviewers(start_offset, table_options)

            if table_name == "workitems":
                return self._read_workitems(start_offset, table_options)

            if table_name == "workitem_revisions":
                return self._read_workitem_revisions(start_offset, table_options)

            if table_name == "workitem_types":
                return self._read_workitem_types(start_offset, table_options)

            raise ValueError(f"Unsupported table: {table_name!r}")

        def _read_projects(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the `projects` snapshot table.

            This implementation lists all projects in the configured organization
            using the Azure DevOps REST API:

                GET /{organization}/_apis/projects?api-version=7.1

            The returned JSON objects are enriched with connector-derived fields:
                - organization: The organization name from connection config.
            """
            url = f"{self.base_url}/_apis/projects"
            params = {
                "api-version": "7.1",
            }

            response = self._session.get(url, params=params, timeout=30)
            if response.status_code != 200:
                raise RuntimeError(
                    f"Azure DevOps API error for projects: {response.status_code} {response.text}"
                )

            response_json = response.json()
            if not isinstance(response_json, dict):
                raise ValueError(
                    f"Unexpected response format for projects: {type(response_json).__name__}"
                )

            projects = response_json.get("value", [])
            if not isinstance(projects, list):
                raise ValueError(
                    f"Unexpected 'value' format in projects response: {type(projects).__name__}"
                )

            records: list[dict[str, Any]] = []
            for project_obj in projects:
                # Shallow-copy the raw JSON and add connector-derived fields
                record: dict[str, Any] = dict(project_obj)
                record["organization"] = self.organization

                records.append(record)

            # Snapshot ingestion: return empty offset
            return iter(records), {}

        def _read_repositories(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the `repositories` snapshot table.

            This implementation lists all Git repositories using the Azure DevOps REST API:

                GET /{organization}/{project}/_apis/git/repositories?api-version=7.1

            Project resolution:
            - If `project` is in table_options, use that project
            - Else if self.project is set (connection-level), use that
            - Else fetch from all projects in the organization (auto-discovery)

            Note: API version 7.1 is used instead of 7.2 to avoid preview version
            requirements. Version 7.2 requires the -preview flag as of this implementation.

            The returned JSON objects are enriched with connector-derived fields:
                - organization: The organization name from connection config.
                - project_name: The project name from the fetched data.
            """
            # Determine which project to use
            project = table_options.get("project") or self.project

            # If no project specified, fetch from all projects
            if not project:
                return self._read_repositories_all_projects(start_offset, table_options)

            url = f"{self.base_url}/{project}/_apis/git/repositories"
            params = {
                "api-version": "7.1",
                "includeLinks": "true",
                "includeAllUrls": "true",
            }

            response = self._session.get(url, params=params, timeout=30)
            if response.status_code != 200:
                raise RuntimeError(
                    f"Azure DevOps API error for repositories: {response.status_code} {response.text}"
                )

            response_json = response.json()
            if not isinstance(response_json, dict):
                raise ValueError(
                    f"Unexpected response format for repositories: {type(response_json).__name__}"
                )

            repos = response_json.get("value", [])
            if not isinstance(repos, list):
                raise ValueError(
                    f"Unexpected 'value' format in repositories response: {type(repos).__name__}"
                )

            records: list[dict[str, Any]] = []
            for repo_obj in repos:
                # Shallow-copy the raw JSON and add connector-derived fields
                record: dict[str, Any] = dict(repo_obj)
                record["organization"] = self.organization
                record["project_name"] = project

                # Ensure nested structs that are absent are represented as None, not {}
                if "parentRepository" not in record or record["parentRepository"] == {}:
                    record["parentRepository"] = None
                if "project" not in record or record["project"] == {}:
                    record["project"] = None
                if "_links" not in record or record["_links"] == {}:
                    record["_links"] = None

                records.append(record)

            # Snapshot ingestion: return empty offset
            return iter(records), {}

        def _read_repositories_all_projects(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Helper method to fetch repositories from ALL projects in the organization.

            This is called when no project is specified at connection or table level.
            """
            # First, get all projects
            projects_iterator, _ = self._read_projects({}, {})
            projects = list(projects_iterator)

            all_records: list[dict[str, Any]] = []

            for project_obj in projects:
                project_name = project_obj.get("name")
                if not project_name:
                    continue

                # Fetch repositories for this project
                try:
                    # Create a temporary table_options with this project
                    temp_options = dict(table_options)
                    temp_options["project"] = project_name

                    # Recursively call _read_repositories with this specific project
                    repos_iterator, _ = self._read_repositories({}, temp_options)
                    all_records.extend(list(repos_iterator))
                except Exception:
                    # Skip projects that fail (might not have Git enabled)
                    continue

            return iter(all_records), {}

        def _read_commits(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the `commits` append table.

            Table options:
            - repository_id (optional): Repository ID or name to fetch commits from.
              If not provided, fetches commits from ALL repositories in the project.

            Supports pagination using $skip offset from start_offset.
            """
            repository_id = table_options.get("repository_id")

            # If repository_id not provided, fetch from all repositories
            if not repository_id:
                return self._read_commits_all_repos(start_offset, table_options)

            # Determine which project to use
            project = table_options.get("project") or self.project
            if not project:
                raise ValueError("Project must be specified either at connection level or in table_options when repository_id is provided")

            url = f"{self.base_url}/{project}/_apis/git/repositories/{repository_id}/commits"

            # Get pagination offset from start_offset
            skip = start_offset.get("skip", 0)
            top = 1000  # Fetch 1000 commits per page

            params = {
                "api-version": "7.1",
                "$top": str(top),
                "$skip": str(skip),
            }

            response = self._session.get(url, params=params, timeout=30)
            if response.status_code != 200:
                raise RuntimeError(
                    f"Azure DevOps API error for commits: {response.status_code} {response.text}"
                )

            response_json = response.json()
            if not isinstance(response_json, dict):
                raise ValueError(
                    f"Unexpected response format for commits: {type(response_json).__name__}"
                )

            commits = response_json.get("value", [])
            if not isinstance(commits, list):
                raise ValueError(
                    f"Unexpected 'value' format in commits response: {type(commits).__name__}"
                )

            records: list[dict[str, Any]] = []
            for commit_obj in commits:
                record: dict[str, Any] = dict(commit_obj)
                record["organization"] = self.organization
                record["project_name"] = project
                record["repository_id"] = repository_id

                # Ensure nested structs are None if absent
                if "author" not in record or record["author"] == {}:
                    record["author"] = None
                if "committer" not in record or record["committer"] == {}:
                    record["committer"] = None
                if "changeCounts" not in record or record["changeCounts"] == {}:
                    record["changeCounts"] = None

                records.append(record)

            # Update offset for next page
            new_offset = {}
            if len(commits) == top:
                # More data likely available
                new_offset["skip"] = skip + top

            return iter(records), new_offset

        def _read_commits_all_repos(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read commits from ALL repositories in the project.

            This method:
            1. Fetches all repositories
            2. For each repository, fetches commits
            3. Combines all commits into a single result set
            """
            # First, get all repositories
            repos_iterator, _ = self._read_repositories({}, {})
            repositories = list(repos_iterator)

            if not repositories:
                # No repositories found, return empty
                return iter([]), {}

            all_commits = []

            # For each repository, fetch commits
            for repo in repositories:
                repo_id = repo.get("id")
                project_name = repo.get("project_name")  # Get project from repo
                if not repo_id or not project_name:
                    continue

                # Create modified table_options with this repository_id and project
                repo_table_options = dict(table_options)
                repo_table_options["repository_id"] = repo_id
                repo_table_options["project"] = project_name  # Add project from repo

                # Fetch commits for this repository
                try:
                    commits_iterator, _ = self._read_commits(start_offset, repo_table_options)
                    commits = list(commits_iterator)
                    all_commits.extend(commits)
                except Exception:
                    # If a repository fails, continue with others
                    # (e.g., empty repo, permissions issue)
                    continue

            # Return all commits combined
            return iter(all_commits), {}

        def _read_pullrequests(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the `pullrequests` CDC table.

            Table options:
            - repository_id (optional): Repository ID or name to fetch pull requests from.
              If not provided, fetches pull requests from ALL repositories in the project.
            - status_filter (optional): Filter by status (active, completed, abandoned, all). Default: all.
            """
            repository_id = table_options.get("repository_id")

            # If repository_id not provided, fetch from all repositories
            if not repository_id:
                return self._read_pullrequests_all_repos(start_offset, table_options)

            # Determine which project to use
            project = table_options.get("project") or self.project
            if not project:
                raise ValueError("Project must be specified either at connection level or in table_options when repository_id is provided")

            url = f"{self.base_url}/{project}/_apis/git/repositories/{repository_id}/pullrequests"

            status_filter = table_options.get("status_filter", "all")

            params = {
                "api-version": "7.1",
                "searchCriteria.status": status_filter,
            }

            response = self._session.get(url, params=params, timeout=30)
            if response.status_code != 200:
                raise RuntimeError(
                    f"Azure DevOps API error for pullrequests: {response.status_code} {response.text}"
                )

            response_json = response.json()
            if not isinstance(response_json, dict):
                raise ValueError(
                    f"Unexpected response format for pullrequests: {type(response_json).__name__}"
                )

            prs = response_json.get("value", [])
            if not isinstance(prs, list):
                raise ValueError(
                    f"Unexpected 'value' format in pullrequests response: {type(prs).__name__}"
                )

            records: list[dict[str, Any]] = []
            for pr_obj in prs:
                record: dict[str, Any] = dict(pr_obj)
                record["organization"] = self.organization
                record["project_name"] = project
                record["repository_id"] = repository_id

                # Ensure nested structs are None if absent
                if "createdBy" not in record or record["createdBy"] == {}:
                    record["createdBy"] = None
                if "lastMergeSourceCommit" not in record or record["lastMergeSourceCommit"] == {}:
                    record["lastMergeSourceCommit"] = None
                if "lastMergeTargetCommit" not in record or record["lastMergeTargetCommit"] == {}:
                    record["lastMergeTargetCommit"] = None
                if "lastMergeCommit" not in record or record["lastMergeCommit"] == {}:
                    record["lastMergeCommit"] = None

                records.append(record)

            # CDC ingestion: return empty offset (full fetch each time)
            return iter(records), {}

        def _read_pullrequests_all_repos(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read pull requests from ALL repositories in the project.

            This method:
            1. Fetches all repositories
            2. For each repository, fetches pull requests
            3. Combines all PRs into a single result set
            """
            # First, get all repositories
            repos_iterator, _ = self._read_repositories({}, {})
            repositories = list(repos_iterator)

            if not repositories:
                # No repositories found, return empty
                return iter([]), {}

            all_prs = []

            # For each repository, fetch pull requests
            for repo in repositories:
                repo_id = repo.get("id")
                project_name = repo.get("project_name")  # Get project from repo
                if not repo_id or not project_name:
                    continue

                # Create modified table_options with this repository_id and project
                repo_table_options = dict(table_options)
                repo_table_options["repository_id"] = repo_id
                repo_table_options["project"] = project_name  # Add project from repo

                # Fetch PRs for this repository
                try:
                    prs_iterator, _ = self._read_pullrequests(start_offset, repo_table_options)
                    prs = list(prs_iterator)
                    all_prs.extend(prs)
                except Exception:
                    # If a repository fails, continue with others
                    continue

            # Return all PRs combined
            return iter(all_prs), {}

        def _read_refs(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the `refs` snapshot table.

            Table options:
            - repository_id (optional): Repository ID or name to fetch refs from.
              If not provided, fetches refs from ALL repositories in the project.
            - filter (optional): Ref name prefix filter (e.g., 'heads/' for branches, 'tags/' for tags).
            """
            repository_id = table_options.get("repository_id")

            # If repository_id not provided, fetch from all repositories
            if not repository_id:
                return self._read_refs_all_repos(start_offset, table_options)

            # Determine which project to use
            project = table_options.get("project") or self.project
            if not project:
                raise ValueError("Project must be specified either at connection level or in table_options when repository_id is provided")

            url = f"{self.base_url}/{project}/_apis/git/repositories/{repository_id}/refs"

            params = {
                "api-version": "7.1",
            }

            # Add optional filter parameter
            ref_filter = table_options.get("filter")
            if ref_filter:
                params["filter"] = ref_filter

            response = self._session.get(url, params=params, timeout=30)
            if response.status_code != 200:
                raise RuntimeError(
                    f"Azure DevOps API error for refs: {response.status_code} {response.text}"
                )

            response_json = response.json()
            if not isinstance(response_json, dict):
                raise ValueError(
                    f"Unexpected response format for refs: {type(response_json).__name__}"
                )

            refs = response_json.get("value", [])
            if not isinstance(refs, list):
                raise ValueError(
                    f"Unexpected 'value' format in refs response: {type(refs).__name__}"
                )

            records: list[dict[str, Any]] = []
            for ref_obj in refs:
                record: dict[str, Any] = dict(ref_obj)
                record["organization"] = self.organization
                record["project_name"] = project
                record["repository_id"] = repository_id

                # Ensure nested structs are None if absent
                if "creator" not in record or record["creator"] == {}:
                    record["creator"] = None

                records.append(record)

            # Snapshot ingestion: return empty offset
            return iter(records), {}

        def _read_refs_all_repos(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read refs from ALL repositories in the project.

            This method:
            1. Fetches all repositories
            2. For each repository, fetches refs
            3. Combines all refs into a single result set
            """
            # First, get all repositories
            repos_iterator, _ = self._read_repositories({}, {})
            repositories = list(repos_iterator)

            if not repositories:
                # No repositories found, return empty
                return iter([]), {}

            all_refs = []

            # For each repository, fetch refs
            for repo in repositories:
                repo_id = repo.get("id")
                project_name = repo.get("project_name")  # Get project from repo
                if not repo_id or not project_name:
                    continue

                # Create modified table_options with this repository_id and project
                repo_table_options = dict(table_options)
                repo_table_options["repository_id"] = repo_id
                repo_table_options["project"] = project_name  # Add project from repo

                # Fetch refs for this repository
                try:
                    refs_iterator, _ = self._read_refs(start_offset, repo_table_options)
                    refs = list(refs_iterator)
                    all_refs.extend(refs)
                except Exception:
                    # If a repository fails, continue with others
                    continue

            # Return all refs combined
            return iter(all_refs), {}

        def _read_pushes(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the `pushes` append table.

            Table options:
            - repository_id (optional): Repository ID or name to fetch pushes from.
              If not provided, fetches pushes from ALL repositories in the project.

            Supports pagination using $skip offset from start_offset.
            """
            repository_id = table_options.get("repository_id")

            # If repository_id not provided, fetch from all repositories
            if not repository_id:
                return self._read_pushes_all_repos(start_offset, table_options)

            # Determine which project to use
            project = table_options.get("project") or self.project
            if not project:
                raise ValueError("Project must be specified either at connection level or in table_options when repository_id is provided")

            url = f"{self.base_url}/{project}/_apis/git/repositories/{repository_id}/pushes"

            # Get pagination offset from start_offset
            skip = start_offset.get("skip", 0)
            top = 1000  # Fetch 1000 pushes per page

            params = {
                "api-version": "7.1",
                "$top": str(top),
                "$skip": str(skip),
            }

            response = self._session.get(url, params=params, timeout=30)
            if response.status_code != 200:
                raise RuntimeError(
                    f"Azure DevOps API error for pushes: {response.status_code} {response.text}"
                )

            response_json = response.json()
            if not isinstance(response_json, dict):
                raise ValueError(
                    f"Unexpected response format for pushes: {type(response_json).__name__}"
                )

            pushes = response_json.get("value", [])
            if not isinstance(pushes, list):
                raise ValueError(
                    f"Unexpected 'value' format in pushes response: {type(pushes).__name__}"
                )

            records: list[dict[str, Any]] = []
            for push_obj in pushes:
                record: dict[str, Any] = dict(push_obj)
                record["organization"] = self.organization
                record["project_name"] = project
                record["repository_id"] = repository_id

                # Ensure nested structs are None if absent
                if "pushedBy" not in record or record["pushedBy"] == {}:
                    record["pushedBy"] = None

                records.append(record)

            # Update offset for next page
            new_offset = {}
            if len(pushes) == top:
                # More data likely available
                new_offset["skip"] = skip + top

            return iter(records), new_offset

        def _read_pushes_all_repos(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read pushes from ALL repositories in the project.

            This method:
            1. Fetches all repositories
            2. For each repository, fetches pushes
            3. Combines all pushes into a single result set
            """
            # First, get all repositories
            repos_iterator, _ = self._read_repositories({}, {})
            repositories = list(repos_iterator)

            if not repositories:
                # No repositories found, return empty
                return iter([]), {}

            all_pushes = []

            # For each repository, fetch pushes
            for repo in repositories:
                repo_id = repo.get("id")
                project_name = repo.get("project_name")  # Get project from repo
                if not repo_id or not project_name:
                    continue

                # Create modified table_options with this repository_id and project
                repo_table_options = dict(table_options)
                repo_table_options["repository_id"] = repo_id
                repo_table_options["project"] = project_name  # Add project from repo

                # Fetch pushes for this repository
                try:
                    pushes_iterator, _ = self._read_pushes(start_offset, repo_table_options)
                    pushes = list(pushes_iterator)
                    all_pushes.extend(pushes)
                except Exception:
                    # If a repository fails, continue with others
                    continue

            # Return all pushes combined
            return iter(all_pushes), {}

        def _read_users(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the `users` snapshot table.

            This implementation lists all user identities in the Azure DevOps organization
            using the Graph API:

                GET https://vssps.dev.azure.com/{organization}/_apis/graph/users?api-version=7.1-preview.1

            Note: This API uses a different base URL (vssps.dev.azure.com) and operates at
            organization level (does not require project parameter).

            The API uses continuation token-based pagination via response headers.

            The returned JSON objects are enriched with connector-derived fields:
                - organization: The organization name from connection config.
            """
            url = f"{self.vssps_base_url}/_apis/graph/users"
            params = {
                "api-version": "7.1-preview.1",
            }

            # Add continuation token from start_offset if present
            continuation_token = (start_offset or {}).get("continuationToken")
            if continuation_token:
                params["continuationToken"] = continuation_token

            response = self._session.get(url, params=params, timeout=30)
            if response.status_code != 200:
                raise RuntimeError(
                    f"Azure DevOps API error for users: {response.status_code} {response.text}"
                )

            response_json = response.json()
            if not isinstance(response_json, dict):
                raise ValueError(
                    f"Unexpected response format for users: {type(response_json).__name__}"
                )

            users = response_json.get("value", [])
            if not isinstance(users, list):
                raise ValueError(
                    f"Unexpected 'value' format in users response: {type(users).__name__}"
                )

            records: list[dict[str, Any]] = []
            for user_obj in users:
                # Shallow-copy the raw JSON and add connector-derived fields
                record: dict[str, Any] = dict(user_obj)
                record["organization"] = self.organization

                # Ensure nested structs that are absent are represented as None, not {}
                if "_links" not in record or record["_links"] == {}:
                    record["_links"] = None

                records.append(record)

            # Check for continuation token in response headers
            new_offset = {}
            next_token = response.headers.get("X-MS-ContinuationToken")
            if next_token:
                # More data available, return continuation token
                new_offset["continuationToken"] = next_token

            # Snapshot ingestion: return empty offset when no more pages
            return iter(records), new_offset

        def _read_pullrequest_threads(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the `pullrequest_threads` append table.

            This implementation fetches comment threads for pull requests. Supports auto-discovery:
            - If repository_id and pullrequest_id provided: fetch threads for specific PR
            - If only repository_id provided: fetch threads for all PRs in that repository
            - If neither provided: fetch threads for all PRs across all repositories

            GET /{organization}/{project}/_apis/git/repositories/{repositoryId}/pullRequests/{pullRequestId}/threads?api-version=7.1
            """
            repository_id = table_options.get("repository_id")
            pullrequest_id = table_options.get("pullrequest_id")

            # Determine which project to use
            project = table_options.get("project") or self.project

            # If no project specified, fetch from all projects
            if not project:
                projects_iterator, _ = self._read_projects({}, {})
                projects = [p.get("name") for p in projects_iterator if p.get("name")]
            else:
                projects = [project]

            all_records = []

            for project in projects:
                if repository_id and pullrequest_id:
                    # Targeted query: specific PR
                    url = f"{self.base_url}/{project}/_apis/git/repositories/{repository_id}/pullRequests/{pullrequest_id}/threads"
                    params = {"api-version": "7.1"}

                    response = self._session.get(url, params=params, timeout=30)
                    if response.status_code != 200:
                        raise RuntimeError(
                            f"Azure DevOps API error for pullrequest_threads: {response.status_code} {response.text}"
                        )

                    threads_data = response.json()
                    threads = threads_data.get("value", [])

                    for thread in threads:
                        record = dict(thread)
                        record["organization"] = self.organization
                        record["project_name"] = project
                        record["repository_id"] = repository_id
                        record["pullrequest_id"] = int(pullrequest_id)
                        all_records.append(record)

                elif repository_id:
                    # Auto-fetch: all PRs in this repository
                    prs_url = f"{self.base_url}/{project}/_apis/git/repositories/{repository_id}/pullrequests"
                    prs_response = self._session.get(
                        prs_url, params={"api-version": "7.1", "searchCriteria.status": "all"}, timeout=30
                    )
                    if prs_response.status_code != 200:
                        continue

                    prs_data = prs_response.json()
                    prs = prs_data.get("value", [])

                    for pr in prs:
                        pr_id = pr.get("pullRequestId")
                        if not pr_id:
                            continue

                        threads_url = f"{self.base_url}/{project}/_apis/git/repositories/{repository_id}/pullRequests/{pr_id}/threads"
                        threads_response = self._session.get(threads_url, params={"api-version": "7.1"}, timeout=30)
                        if threads_response.status_code != 200:
                            continue

                        threads_data = threads_response.json()
                        threads = threads_data.get("value", [])

                        for thread in threads:
                            record = dict(thread)
                            record["organization"] = self.organization
                            record["project_name"] = project
                            record["repository_id"] = repository_id
                            record["pullrequest_id"] = pr_id
                            all_records.append(record)

                else:
                    # Auto-fetch: all PRs across all repositories
                    repos_url = f"{self.base_url}/{project}/_apis/git/repositories"
                    repos_response = self._session.get(repos_url, params={"api-version": "7.1"}, timeout=30)
                    if repos_response.status_code != 200:
                        continue

                    repos_data = repos_response.json()
                    repos = repos_data.get("value", [])

                    for repo in repos:
                        repo_id = repo.get("id")
                        if not repo_id:
                            continue

                        prs_url = f"{self.base_url}/{project}/_apis/git/repositories/{repo_id}/pullrequests"
                        prs_response = self._session.get(
                            prs_url, params={"api-version": "7.1", "searchCriteria.status": "all"}, timeout=30
                        )
                        if prs_response.status_code != 200:
                            continue

                        prs_data = prs_response.json()
                        prs = prs_data.get("value", [])

                        for pr in prs:
                            pr_id = pr.get("pullRequestId")
                            if not pr_id:
                                continue

                            threads_url = f"{self.base_url}/{project}/_apis/git/repositories/{repo_id}/pullRequests/{pr_id}/threads"
                            threads_response = self._session.get(threads_url, params={"api-version": "7.1"}, timeout=30)
                            if threads_response.status_code != 200:
                                continue

                            threads_data = threads_response.json()
                            threads = threads_data.get("value", [])

                            for thread in threads:
                                record = dict(thread)
                                record["organization"] = self.organization
                                record["project_name"] = project
                                record["repository_id"] = repo_id
                                record["pullrequest_id"] = pr_id
                                all_records.append(record)

            return iter(all_records), {}

        def _read_pullrequest_workitems(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the `pullrequest_workitems` snapshot table.

            This implementation fetches work items linked to pull requests.

            GET /{organization}/{project}/_apis/git/repositories/{repositoryId}/pullRequests/{pullRequestId}/workitems?api-version=7.1
            """
            repository_id = table_options.get("repository_id")
            pullrequest_id = table_options.get("pullrequest_id")

            # Determine which project to use
            project = table_options.get("project") or self.project

            # If no project specified, fetch from all projects
            if not project:
                projects_iterator, _ = self._read_projects({}, {})
                projects = [p.get("name") for p in projects_iterator if p.get("name")]
            else:
                projects = [project]

            all_records = []

            for project in projects:
                if repository_id and pullrequest_id:
                    # Targeted query: specific PR
                    url = f"{self.base_url}/{project}/_apis/git/repositories/{repository_id}/pullRequests/{pullrequest_id}/workitems"
                    params = {"api-version": "7.1"}

                    response = self._session.get(url, params=params, timeout=30)
                    if response.status_code != 200:
                        raise RuntimeError(
                            f"Azure DevOps API error for pullrequest_workitems: {response.status_code} {response.text}"
                        )

                    # Response is a flat array, not wrapped in {value: [...]}
                    workitems = response.json()
                    if not isinstance(workitems, list):
                        workitems = []

                    for workitem in workitems:
                        record = dict(workitem)
                        record["organization"] = self.organization
                        record["project_name"] = project
                        record["repository_id"] = repository_id
                        record["pullrequest_id"] = int(pullrequest_id)
                        all_records.append(record)

                elif repository_id:
                    # Auto-fetch: all PRs in this repository
                    prs_url = f"{self.base_url}/{project}/_apis/git/repositories/{repository_id}/pullrequests"
                    prs_response = self._session.get(
                        prs_url, params={"api-version": "7.1", "searchCriteria.status": "all"}, timeout=30
                    )
                    if prs_response.status_code != 200:
                        continue

                    prs_data = prs_response.json()
                    prs = prs_data.get("value", [])

                    for pr in prs:
                        pr_id = pr.get("pullRequestId")
                        if not pr_id:
                            continue

                        workitems_url = f"{self.base_url}/{project}/_apis/git/repositories/{repository_id}/pullRequests/{pr_id}/workitems"
                        workitems_response = self._session.get(workitems_url, params={"api-version": "7.1"}, timeout=30)
                        if workitems_response.status_code != 200:
                            continue

                        workitems = workitems_response.json()
                        if not isinstance(workitems, list):
                            continue

                        for workitem in workitems:
                            record = dict(workitem)
                            record["organization"] = self.organization
                            record["project_name"] = project
                            record["repository_id"] = repository_id
                            record["pullrequest_id"] = pr_id
                            all_records.append(record)

                else:
                    # Auto-fetch: all PRs across all repositories
                    repos_url = f"{self.base_url}/{project}/_apis/git/repositories"
                    repos_response = self._session.get(repos_url, params={"api-version": "7.1"}, timeout=30)
                    if repos_response.status_code != 200:
                        continue

                    repos_data = repos_response.json()
                    repos = repos_data.get("value", [])

                    for repo in repos:
                        repo_id = repo.get("id")
                        if not repo_id:
                            continue

                        prs_url = f"{self.base_url}/{project}/_apis/git/repositories/{repo_id}/pullrequests"
                        prs_response = self._session.get(
                            prs_url, params={"api-version": "7.1", "searchCriteria.status": "all"}, timeout=30
                        )
                        if prs_response.status_code != 200:
                            continue

                        prs_data = prs_response.json()
                        prs = prs_data.get("value", [])

                        for pr in prs:
                            pr_id = pr.get("pullRequestId")
                            if not pr_id:
                                continue

                            workitems_url = f"{self.base_url}/{project}/_apis/git/repositories/{repo_id}/pullRequests/{pr_id}/workitems"
                            workitems_response = self._session.get(workitems_url, params={"api-version": "7.1"}, timeout=30)
                            if workitems_response.status_code != 200:
                                continue

                            workitems = workitems_response.json()
                            if not isinstance(workitems, list):
                                continue

                            for workitem in workitems:
                                record = dict(workitem)
                                record["organization"] = self.organization
                                record["project_name"] = project
                                record["repository_id"] = repo_id
                                record["pullrequest_id"] = pr_id
                                all_records.append(record)

            return iter(all_records), {}

        def _read_pullrequest_commits(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the `pullrequest_commits` snapshot table.

            This implementation fetches commits included in pull requests.

            GET /{organization}/{project}/_apis/git/repositories/{repositoryId}/pullRequests/{pullRequestId}/commits?api-version=7.1
            """
            repository_id = table_options.get("repository_id")
            pullrequest_id = table_options.get("pullrequest_id")

            # Determine which project to use
            project = table_options.get("project") or self.project

            # If no project specified, fetch from all projects
            if not project:
                projects_iterator, _ = self._read_projects({}, {})
                projects = [p.get("name") for p in projects_iterator if p.get("name")]
            else:
                projects = [project]

            all_records = []

            for project in projects:
                if repository_id and pullrequest_id:
                    # Targeted query: specific PR
                    url = f"{self.base_url}/{project}/_apis/git/repositories/{repository_id}/pullRequests/{pullrequest_id}/commits"
                    params = {"api-version": "7.1"}

                    response = self._session.get(url, params=params, timeout=30)
                    if response.status_code != 200:
                        raise RuntimeError(
                            f"Azure DevOps API error for pullrequest_commits: {response.status_code} {response.text}"
                        )

                    commits_data = response.json()
                    commits = commits_data.get("value", [])

                    for commit in commits:
                        record = dict(commit)
                        record["organization"] = self.organization
                        record["project_name"] = project
                        record["repository_id"] = repository_id
                        record["pullrequest_id"] = int(pullrequest_id)
                        all_records.append(record)

                elif repository_id:
                    # Auto-fetch: all PRs in this repository
                    prs_url = f"{self.base_url}/{project}/_apis/git/repositories/{repository_id}/pullrequests"
                    prs_response = self._session.get(
                        prs_url, params={"api-version": "7.1", "searchCriteria.status": "all"}, timeout=30
                    )
                    if prs_response.status_code != 200:
                        continue

                    prs_data = prs_response.json()
                    prs = prs_data.get("value", [])

                    for pr in prs:
                        pr_id = pr.get("pullRequestId")
                        if not pr_id:
                            continue

                        commits_url = f"{self.base_url}/{project}/_apis/git/repositories/{repository_id}/pullRequests/{pr_id}/commits"
                        commits_response = self._session.get(commits_url, params={"api-version": "7.1"}, timeout=30)
                        if commits_response.status_code != 200:
                            continue

                        commits_data = commits_response.json()
                        commits = commits_data.get("value", [])

                        for commit in commits:
                            record = dict(commit)
                            record["organization"] = self.organization
                            record["project_name"] = project
                            record["repository_id"] = repository_id
                            record["pullrequest_id"] = pr_id
                            all_records.append(record)

                else:
                    # Auto-fetch: all PRs across all repositories
                    repos_url = f"{self.base_url}/{project}/_apis/git/repositories"
                    repos_response = self._session.get(repos_url, params={"api-version": "7.1"}, timeout=30)
                    if repos_response.status_code != 200:
                        continue

                    repos_data = repos_response.json()
                    repos = repos_data.get("value", [])

                    for repo in repos:
                        repo_id = repo.get("id")
                        if not repo_id:
                            continue

                        prs_url = f"{self.base_url}/{project}/_apis/git/repositories/{repo_id}/pullrequests"
                        prs_response = self._session.get(
                            prs_url, params={"api-version": "7.1", "searchCriteria.status": "all"}, timeout=30
                        )
                        if prs_response.status_code != 200:
                            continue

                        prs_data = prs_response.json()
                        prs = prs_data.get("value", [])

                        for pr in prs:
                            pr_id = pr.get("pullRequestId")
                            if not pr_id:
                                continue

                            commits_url = f"{self.base_url}/{project}/_apis/git/repositories/{repo_id}/pullRequests/{pr_id}/commits"
                            commits_response = self._session.get(commits_url, params={"api-version": "7.1"}, timeout=30)
                            if commits_response.status_code != 200:
                                continue

                            commits_data = commits_response.json()
                            commits = commits_data.get("value", [])

                            for commit in commits:
                                record = dict(commit)
                                record["organization"] = self.organization
                                record["project_name"] = project
                                record["repository_id"] = repo_id
                                record["pullrequest_id"] = pr_id
                                all_records.append(record)

            return iter(all_records), {}

        def _read_pullrequest_reviewers(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the `pullrequest_reviewers` cdc table.

            This implementation fetches reviewer information for pull requests.

            GET /{organization}/{project}/_apis/git/repositories/{repositoryId}/pullRequests/{pullRequestId}/reviewers?api-version=7.1
            """
            repository_id = table_options.get("repository_id")
            pullrequest_id = table_options.get("pullrequest_id")

            # Determine which project to use
            project = table_options.get("project") or self.project

            # If no project specified, fetch from all projects
            if not project:
                projects_iterator, _ = self._read_projects({}, {})
                projects = [p.get("name") for p in projects_iterator if p.get("name")]
            else:
                projects = [project]

            all_records = []

            for project in projects:
                if repository_id and pullrequest_id:
                    # Targeted query: specific PR
                    url = f"{self.base_url}/{project}/_apis/git/repositories/{repository_id}/pullRequests/{pullrequest_id}/reviewers"
                    params = {"api-version": "7.1"}

                    response = self._session.get(url, params=params, timeout=30)
                    if response.status_code != 200:
                        raise RuntimeError(
                            f"Azure DevOps API error for pullrequest_reviewers: {response.status_code} {response.text}"
                        )

                    # Response is a flat array
                    reviewers = response.json()
                    if not isinstance(reviewers, list):
                        reviewers = []

                    for reviewer in reviewers:
                        record = dict(reviewer)
                        record["organization"] = self.organization
                        record["project_name"] = project
                        record["repository_id"] = repository_id
                        record["pullrequest_id"] = int(pullrequest_id)
                        all_records.append(record)

                elif repository_id:
                    # Auto-fetch: all PRs in this repository
                    prs_url = f"{self.base_url}/{project}/_apis/git/repositories/{repository_id}/pullrequests"
                    prs_response = self._session.get(
                        prs_url, params={"api-version": "7.1", "searchCriteria.status": "all"}, timeout=30
                    )
                    if prs_response.status_code != 200:
                        continue

                    prs_data = prs_response.json()
                    prs = prs_data.get("value", [])

                    for pr in prs:
                        pr_id = pr.get("pullRequestId")
                        if not pr_id:
                            continue

                        reviewers_url = f"{self.base_url}/{project}/_apis/git/repositories/{repository_id}/pullRequests/{pr_id}/reviewers"
                        reviewers_response = self._session.get(reviewers_url, params={"api-version": "7.1"}, timeout=30)
                        if reviewers_response.status_code != 200:
                            continue

                        reviewers = reviewers_response.json()
                        if not isinstance(reviewers, list):
                            continue

                        for reviewer in reviewers:
                            record = dict(reviewer)
                            record["organization"] = self.organization
                            record["project_name"] = project
                            record["repository_id"] = repository_id
                            record["pullrequest_id"] = pr_id
                            all_records.append(record)

                else:
                    # Auto-fetch: all PRs across all repositories
                    repos_url = f"{self.base_url}/{project}/_apis/git/repositories"
                    repos_response = self._session.get(repos_url, params={"api-version": "7.1"}, timeout=30)
                    if repos_response.status_code != 200:
                        continue

                    repos_data = repos_response.json()
                    repos = repos_data.get("value", [])

                    for repo in repos:
                        repo_id = repo.get("id")
                        if not repo_id:
                            continue

                        prs_url = f"{self.base_url}/{project}/_apis/git/repositories/{repo_id}/pullrequests"
                        prs_response = self._session.get(
                            prs_url, params={"api-version": "7.1", "searchCriteria.status": "all"}, timeout=30
                        )
                        if prs_response.status_code != 200:
                            continue

                        prs_data = prs_response.json()
                        prs = prs_data.get("value", [])

                        for pr in prs:
                            pr_id = pr.get("pullRequestId")
                            if not pr_id:
                                continue

                            reviewers_url = f"{self.base_url}/{project}/_apis/git/repositories/{repo_id}/pullRequests/{pr_id}/reviewers"
                            reviewers_response = self._session.get(reviewers_url, params={"api-version": "7.1"}, timeout=30)
                            if reviewers_response.status_code != 200:
                                continue

                            reviewers = reviewers_response.json()
                            if not isinstance(reviewers, list):
                                continue

                            for reviewer in reviewers:
                                record = dict(reviewer)
                                record["organization"] = self.organization
                                record["project_name"] = project
                                record["repository_id"] = repo_id
                                record["pullrequest_id"] = pr_id
                                all_records.append(record)

            return iter(all_records), {}

        def _read_workitems(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the `workitems` cdc table.

            If 'ids' is provided in table_options, fetches those specific work items.
            If 'ids' is not provided, uses WIQL to discover and fetch all work items.

            GET /{organization}/{project}/_apis/wit/workitems?ids={ids}&api-version=7.1
            POST /{organization}/{project}/_apis/wit/wiql?api-version=7.1
            """
            ids = table_options.get("ids")

            # Determine which project to use
            project = table_options.get("project") or self.project

            # If no project specified, fetch from all projects
            if not project:
                projects_iterator, _ = self._read_projects({}, {})
                projects = [p.get("name") for p in projects_iterator if p.get("name")]
            else:
                projects = [project]

            all_records = []

            for project in projects:
                # If no IDs provided, use WIQL to discover all work items
                if not ids:
                    wiql_url = f"{self.base_url}/{project}/_apis/wit/wiql"
                    wiql_query = {
                        "query": "SELECT [System.Id] FROM WorkItems"
                    }
                    wiql_response = self._session.post(
                        wiql_url,
                        json=wiql_query,
                        params={"api-version": "7.1"},
                        timeout=30
                    )
                    if wiql_response.status_code != 200:
                        raise RuntimeError(
                            f"Azure DevOps API error for workitems WIQL: {wiql_response.status_code} {wiql_response.text}"
                        )

                    wiql_results = wiql_response.json()
                    work_item_refs = wiql_results.get("workItems", [])

                    # If no work items found in this project, skip
                    if not work_item_refs:
                        continue

                    # Extract IDs from WIQL results
                    discovered_ids = [str(ref.get("id")) for ref in work_item_refs if ref.get("id")]
                    ids_to_fetch = ",".join(discovered_ids)
                else:
                    ids_to_fetch = ids

                # Fetch work items by IDs
                url = f"{self.base_url}/{project}/_apis/wit/workitems"
                params = {
                    "api-version": "7.1",
                    "ids": ids_to_fetch,
                    "$expand": "relations",
                }

                response = self._session.get(url, params=params, timeout=30)
                if response.status_code != 200:
                    raise RuntimeError(
                        f"Azure DevOps API error for workitems: {response.status_code} {response.text}"
                    )

                workitems_data = response.json()
                workitems = workitems_data.get("value", [])

                for workitem in workitems:
                    record = dict(workitem)
                    record["organization"] = self.organization
                    record["project_name"] = project

                    # Convert fields dict to JSON string for storage
                    import json
                    if "fields" in record and isinstance(record["fields"], dict):
                        record["fields"] = json.dumps(record["fields"])

                    all_records.append(record)

            return iter(all_records), {}

        def _read_workitem_revisions(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the `workitem_revisions` append table.

            This implementation fetches all revisions of work items incrementally using continuation tokens.

            GET /{organization}/{project}/_apis/wit/reporting/workitemrevisions?api-version=7.1
            """
            # Determine which project to use
            project = table_options.get("project") or self.project

            # If no project specified, fetch from all projects
            if not project:
                projects_iterator, _ = self._read_projects({}, {})
                projects = [p.get("name") for p in projects_iterator if p.get("name")]
            else:
                projects = [project]

            all_records = []

            for project in projects:
                url = f"{self.base_url}/{project}/_apis/wit/reporting/workitemrevisions"
                params = {
                    "api-version": "7.1",
                    "includeDeleted": "true",
                }

                # Add continuation token from start_offset if present
                continuation_token = (start_offset or {}).get("continuationToken")
                if continuation_token:
                    params["continuationToken"] = continuation_token

                response = self._session.get(url, params=params, timeout=30)
                if response.status_code != 200:
                    raise RuntimeError(
                        f"Azure DevOps API error for workitem_revisions: {response.status_code} {response.text}"
                    )

                revisions_data = response.json()
                revisions = revisions_data.get("values", [])

                for revision in revisions:
                    record = dict(revision)
                    record["organization"] = self.organization
                    record["project_name"] = project

                    # Convert fields dict to JSON string for storage
                    import json
                    if "fields" in record and isinstance(record["fields"], dict):
                        record["fields"] = json.dumps(record["fields"])

                    all_records.append(record)

                # Check for continuation token
                new_offset = {}
                next_token = revisions_data.get("continuationToken")
                if next_token:
                    new_offset["continuationToken"] = next_token
                    return iter(all_records), new_offset

            return iter(all_records), {}

        def _read_workitem_types(
            self, start_offset: dict, table_options: dict[str, str]
        ) -> (Iterator[dict], dict):
            """
            Read the `workitem_types` snapshot table.

            This implementation fetches all work item type definitions for a project.

            GET /{organization}/{project}/_apis/wit/workitemtypes?api-version=7.1
            """
            # Determine which project to use
            project = table_options.get("project") or self.project

            # If no project specified, fetch from all projects
            if not project:
                projects_iterator, _ = self._read_projects({}, {})
                projects = [p.get("name") for p in projects_iterator if p.get("name")]
            else:
                projects = [project]

            all_records = []

            for project in projects:
                url = f"{self.base_url}/{project}/_apis/wit/workitemtypes"
                params = {"api-version": "7.1"}

                response = self._session.get(url, params=params, timeout=30)
                if response.status_code != 200:
                    raise RuntimeError(
                        f"Azure DevOps API error for workitem_types: {response.status_code} {response.text}"
                    )

                types_data = response.json()
                types = types_data.get("value", [])

                for work_type in types:
                    record = dict(work_type)
                    record["organization"] = self.organization
                    record["project_name"] = project
                    all_records.append(record)

            return iter(all_records), {}


    ########################################################
    # pipeline/lakeflow_python_source.py
    ########################################################

    METADATA_TABLE = "_lakeflow_metadata"
    TABLE_NAME = "tableName"
    TABLE_NAME_LIST = "tableNameList"
    TABLE_CONFIGS = "tableConfigs"
    IS_DELETE_FLOW = "isDeleteFlow"


    class LakeflowStreamReader(SimpleDataSourceStreamReader):
        """
        Implements a data source stream reader for Lakeflow Connect.
        Currently, only the simpleStreamReader is implemented, which uses a
        more generic protocol suitable for most data sources that support
        incremental loading.
        """

        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.lakeflow_connect = lakeflow_connect
            self.schema = schema

        def initialOffset(self):
            return {}

        def read(self, start: dict) -> (Iterator[tuple], dict):
            is_delete_flow = self.options.get(IS_DELETE_FLOW) == "true"
            # Strip delete flow options before passing to connector
            table_options = {
                k: v for k, v in self.options.items() if k != IS_DELETE_FLOW
            }

            if is_delete_flow:
                records, offset = self.lakeflow_connect.read_table_deletes(
                    self.options[TABLE_NAME], start, table_options
                )
            else:
                records, offset = self.lakeflow_connect.read_table(
                    self.options[TABLE_NAME], start, table_options
                )
            rows = map(lambda x: parse_value(x, self.schema), records)
            return rows, offset

        def readBetweenOffsets(self, start: dict, end: dict) -> Iterator[tuple]:
            # TODO: This does not ensure the records returned are identical across repeated calls.
            # For append-only tables, the data source must guarantee that reading from the same
            # start offset will always yield the same set of records.
            # For tables ingested as incremental CDC, it is only necessary that no new changes
            # are missed in the returned records.
            return self.read(start)[0]


    class LakeflowBatchReader(DataSourceReader):
        def __init__(
            self,
            options: dict[str, str],
            schema: StructType,
            lakeflow_connect: LakeflowConnect,
        ):
            self.options = options
            self.schema = schema
            self.lakeflow_connect = lakeflow_connect
            self.table_name = options[TABLE_NAME]

        def read(self, partition):
            all_records = []
            if self.table_name == METADATA_TABLE:
                all_records = self._read_table_metadata()
            else:
                all_records, _ = self.lakeflow_connect.read_table(
                    self.table_name, None, self.options
                )

            rows = map(lambda x: parse_value(x, self.schema), all_records)
            return iter(rows)

        def _read_table_metadata(self):
            table_name_list = self.options.get(TABLE_NAME_LIST, "")
            table_names = [o.strip() for o in table_name_list.split(",") if o.strip()]
            all_records = []
            table_configs = json.loads(self.options.get(TABLE_CONFIGS, "{}"))
            for table in table_names:
                metadata = self.lakeflow_connect.read_table_metadata(
                    table, table_configs.get(table, {})
                )
                all_records.append({TABLE_NAME: table, **metadata})
            return all_records


    class LakeflowSource(DataSource):
        def __init__(self, options):
            self.options = options
            self.lakeflow_connect = LakeflowConnect(options)

        @classmethod
        def name(cls):
            return "lakeflow_connect"

        def schema(self):
            table = self.options[TABLE_NAME]
            if table == METADATA_TABLE:
                return StructType(
                    [
                        StructField(TABLE_NAME, StringType(), False),
                        StructField("primary_keys", ArrayType(StringType()), True),
                        StructField("cursor_field", StringType(), True),
                        StructField("ingestion_type", StringType(), True),
                    ]
                )
            else:
                # Assuming the LakeflowConnect interface uses get_table_schema, not get_table_details
                return self.lakeflow_connect.get_table_schema(table, self.options)

        def reader(self, schema: StructType):
            return LakeflowBatchReader(self.options, schema, self.lakeflow_connect)

        def simpleStreamReader(self, schema: StructType):
            return LakeflowStreamReader(self.options, schema, self.lakeflow_connect)


    spark.dataSource.register(LakeflowSource)  # pylint: disable=undefined-variable
